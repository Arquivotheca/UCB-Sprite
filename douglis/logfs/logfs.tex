% Master File: logfs.tex
% Document Type: LaTeX
\documentstyle[times,captions,proc]{article}
\input{defs}
\begin{document}
\pagestyle{empty}
%\setstretch{1}
%\renewcommand{\baselinestretch}{0.9}\large\normalsize
\title{Log-Structured File Systems\thanks{
The work described here was supported in part by the
National Science Foundation under Presidential Young
Investigator Award No. ECS-8351961 and Grant No. MIP-8715235.
}}
%\title{Log-Structured File Systems}
\author{Fred Douglis \hskip 1em plus .17fil John Ousterhout \\
\\ %skip some space
Computer Science Division \\
Electrical Engineering and Computer Sciences \\
University of California \\
Berkeley, CA 94720
}
\maketitle
% redefine copyrightspace not to print a "0"
%\def\copyrightspace{\footnotetext[\empty]{\mbox{}\vrule height 97pt width 0pt}}
% Actually, IEEE doesn't seem to care about extra space for the copyright.
%\copyrightspace
\begin{abstract}
CPU speeds are improving at a dramatic rate, while disk speeds are
not.  This technology shift suggests that many engineering and
office applications may become so I/O-limited that they cannot benefit
from further CPU improvements.  This paper discusses several
techniques for improving I/O performance, including caches,
battery-backed-up caches, and cache logging.  We then examine in
particular detail an approach called {\em log-structured file
systems\/}, where the file system's only representation on disk is in
the form of an append-only log.  Log-structured file systems
potentially provide order-of-magnitude improvements in write
performance.  When log-structured file systems are combined with
arrays of small disks (which provide high bandwidth) and large
main-memory file caches (which satisfy most read accesses), we believe
it will be possible to achieve 1000-fold improvements in I/O
performance over today's systems.
\end{abstract}
\setlength{\parskip}{3pt}
\section{Introduction}
\label{intro}
\hskip\parindent
If a computer program or system consists of several components,
and if some of the components are made much faster while leaving
the other components unchanged, then the unimproved components
are likely to become a performance bottleneck.
%This notion was first
%put forth by Gene Amdahl in the late 1960's, and has since
%come to be known as ``Amdahl's Law''~\cite{amdahls-law}.
%
Recent technology trends suggest that disk output may become such a
bottleneck in the near future, if it is not already.  Between 1985 and 1995, typical
CPU speeds in engineering and office environments are likely
to increase by factors of 100 to 1000.  Memory sizes will also increase
substantially, although probably by a factor of less than 100.
In contrast, disk speeds (particularly seek times) seem unlikely to
improve by more than a factor of 2 or 3 in the same
time period.  This shift will open up
new CPU-intensive applications, but suggests that new I/O techniques
will be needed to keep I/O from being a performance bottleneck.

Fortunately, two technology trends offer hope for
beating the I/O bottleneck: large memories and disk arrays.  First, as
memories get larger and larger, it becomes possible to cache more
and more file information in main memory and thereby eliminate many disk
accesses.  Second, disks are becoming much cheaper and physically
smaller.  This
makes it practical to build disk systems containing tens or
hundreds of drives (``disk arrays'').    
In order to be as reliable as
individual 
disks, such disk arrays are likely to store data redundantly; for
example, Patterson, et al., have proposed a method by which the parity
for  a ``stripe'' across $N-1$ disks of an $N$-disk array is stored on
the remaining 
disk.  Writing any part of the stripe requires rewriting the parity,
so for efficiency, the size of all writes should be multiples of the
size of a stripe (typically one track on each disk)~\cite{pattrsn:raid}.  
Disk arrays don't improve
the performance of a single small access, but they offer much
greater overall bandwidth if transfers are performed in much larger units
than today's UNIX systems~\cite{mckusick:unix42}.


This paper discusses how the technology trends may affect future engineering and office
applications.  Our overall goal
is to find ways of capitalizing on fast-moving technologies
(CPU speed, memory size, disk array size) to compensate for
slow-moving technologies (disk seek times) so that future systems
will not be I/O-limited.  

Section~\ref{scenario} uses information about
file access patterns to compute a worst-case scenario for I/O
requirements of the future.  Section~\ref{caching} discusses file caching;
this is a well-known technique that is already implemented in
several systems, but Section~\ref{caching}
argues that write traffic limits
the performance improvements of caching alone.
Section~\ref{nonvol-cache} discusses ways to
improve file caches by reducing 
write traffic:  battery backup and cache logging.

In Section~\ref{logfs} we make our most radical and detailed proposal, which
is to restructure the file system so that its only representation
on disk is in the form of a circular append-only log.
Such a {\em log-structured file system\/}  eliminates almost all seeks
during writes, and thus may improve writing performance by as much as an
order of magnitude.  The logging approach also makes
it easier to utilize the additional bandwidth provided by disk arrays,
by writing large amounts of data sequentially even in systems in which
file accesses are small.
When combined with a large main-memory file cache, the cache will
satisfy almost all of the read requests while the log-structured disks
will handle writes efficiently.  Overall, we think that this approach
can result in ten times better utilization of disk bandwidth than today's file
systems;  if used with disk arrays containing hundreds of disks,
three orders of magnitude total improvement in file system performance
may be possible.  Log-structured file systems also have other advantages
including faster crash recovery and integration of archival storage.

Section~\ref{related} describes other work with similarities to log-structured
file systems, and Section~\ref{concl} concludes the paper.

\section{The File Access Problem}
\hskip\parindent
\label{scenario}

In this section we attempt to predict how today's file systems
will behave if the technology predictions of Section~\ref{intro}
are accurate
and if no changes are made in file system organization.  
%Section~\ref{patterns}
%discusses file access patterns, and Section~\ref{demands} estimates
%the I/O demands that might be placed on a computing system of the
%mid-1990's.
%
% \subsection{File Access Patterns}
% \hskip\parindent
% \label{patterns}
% 
% In order to understand the impact of technology changes on file system
% performance, one must consider how file systems
% are used by application programs.  We think that file access patterns
% can be separated into three general classes:  scientific computing,
% transaction processing, and engineering/office applications.  Of these
% classes, the first two appear well-suited to disk arrays
% while the third does not.
% 
% The first class of file usage, which we call ``scientific processing'',
% is characterized by programs that read
% and write very large files sequentially.  Many scientific applications
% and some engineering and database applications fall into this category.
% If sequential blocks of large files are spread evenly across
% the disks of an array (``striped''), then all the disks can be used
% simultaneously to read or write the
% files~\cite{kim:disk-interleaving}.  If the files are large 
% enough, then the cost of seeks will be small compared to the cost
% of reading the data, and the file system's performance will scale
% with the size of the array.  Thus if the CPU power
% and number of disks in
% an array scale at about the same rate (which appears likely for the
% next several years), then scientific applications should be
% able to scale smoothly in performance even though the disk latency does
% not improve.
% 
% We use the term ``transaction processing'' to refer
% to applications like airline reservations,
% automatic tellers, and point-of-sale terminals.  These systems
% typically handle large numbers of concurrent requests, each of
% which makes a small number of disk accesses (for example, to
% reserve a seat or make a deposit).
% Transaction processing systems are usually measured by their
% throughput in transactions per second, rather than by the
% service time for individual transactions.
% Different requests usually involve
% different disk data, so it is possible to keep many disks busy
% simultaneously.  This means that transaction processing systems
% will also scale smoothly as long as the number
% of disks in an array increases as fast as CPU power:  the speed
% of an individual transaction will still be limited by latency,
% but the system's overall throughput will increase.
% 
% The third class of file usage, ``engineering/office applications,''
% consists of programs that access large numbers of small files, such
% as the source files for a program or the font libraries for
% a laser printer.  Most programs in UNIX environments fall into
% this class.  These applications are similar to transaction processing
% systems in that they make small requests.   However, they are different
% from transaction processing in that there isn't much concurrency:
% each application program makes a sequence of requests, and each CPU
% typically runs only one or a few application programs at once.
% Furthermore, the goal of increasing the CPU power in an engineering/office
% environment is to make individual programs
% run faster, not to increase the number of concurrent programs.
% As a result, these applications do not appear to benefit from disk
% arrays:  files are short enough that striping doesn't help (most of
% the access cost is in the latency), and there isn't enough concurrency
% to occupy the disks of a large array.
% 
% For the rest of this paper we will focus on engineering and office
% applications.  This is partly because it is less obvious how to
% take advantage of disk array technology for these applications than
% for the others, and partly because we have
% more experience with them than with the other applications.  However,
% some of the results also apply to scientific applications.
% 
%
%\section{How Many Disks per User?}
%\hskip\parindent
%\label{demands}
Suppose that I/O rates generated by application programs scale evenly
with CPU power.  Several recent studies, plus our own measurements,
suggest that the average I/O traffic generated by
a single user on a 1-MIP engineering workstation is about 2.5-10
kbytes/sec.~\cite{lazowska:file-perf,ouster:bsd}.   In the environment
of the mid 1990's with 100-MIP 
workstations, each user might then generate 250-1000 kbytes/sec. of
I/O traffic.

The average file size in engineering/office environments is
only 3-4 kbytes~\cite{ouster:bsd,satya:sizes}.  With today's file
organizations (e.g. 4.3 BSD UNIX~\cite{mckusick:unix42}), two disk
transfers are required 
for each file access:  one to read or write file header information,
and another to read or write the file's data.  Typical disks today
can only make about 30-40 transfers per second.  Thus the
effective disk bandwidth for a single disk is about
\begin{center}
((30-40 transfers/sec.) / (2 transfers/file)) * (3-4 kbytes/file) \\
 $\approx$ 60 kbytes/sec.
\end{center}

If file system organization and average file sizes don't change
over the next five years, then the above calculations suggest that
future systems will need 4-16 disks per user
to keep up with I/O demands.  This has two unpleasant implications.
First, it suggests that disks would represent by far the greatest part
of the cost of a computer system.  Second, it requires the operating
system to organize the file system so that it can keep
4-16 disks busy for each user:  if a user's data ends up all
on a single disk, then the file system will not be able to keep up with
the user's I/O demands.

% Of course, it's possible that I/O access patterns will change as
% CPU power increases.  For example, the extra CPU power of
% future machines might be used primarily for managing the user interface
% (graphics, speech recognition, etc.) and thereby require very little
% additional I/O.  Or, average file sizes might increase and thereby
% allow more efficient utilization of disk bandwidth;  this could happen
% if, for example, future applications make heavy use of large images.
% To the extent that these changes occur, I/O demands will be less
% than we estimated above.
% On the other hand, CPU power might increase to more than 100 MIPs
% by the mid-1990s;  if it increases to 500 MIPs, then as many as
% 20-80 disks per user might be required!  Overall, we think it is
% plausible that disk requirements will be severe enough to present
% difficult problems both in terms of cost and in terms of keeping
% them all busy.

\section{File Caching}
\hskip\parindent
\label{caching}

File caching has been implemented in several systems to improve
file I/O
performance (e.g., ~\cite{mckusick:unix42,nelson:caching-in-sprite}).
The idea in  
file caching is to retain recently-accessed disk blocks
in main memory, so that repeated accesses to the
same information can be handled without additional disk transfers.
Because of locality in file access patterns, file caching
reduces disk I/O substantially:  typical systems today achieve
80-90\% read hit rates with file caches of .5-5 Mbytes.
File servers of the future are likely to have file caches
with hundreds of Mbytes;  this should be enough to hold all the files
used by groups of dozens of users over periods of days or weeks (see~\cite{ouster:bsd} for supporting evidence).  As a result,
almost all read requests should be satisfied in the file cache.

Unfortunately, about 1/3 of file I/O (measured by either files accessed
or bytes transferred) is writes~\cite{ouster:bsd}, and most of this data must
be written to disk.
One possible approach is to place
newly-written data in the cache and delay writes to disk for a while;
some of the newly-written data will be deleted before they are written
to disk, and disk I/O requirements will be reduced correspondingly (for
example, \cite{ouster:bsd} measured that 15-20\% of all newly-written bytes are
deleted within 30 seconds;  in recent measurements of our
current system, we found that 40\% or more of all new bytes lived less
than 30 seconds and 90\% lived less than a day).  However, the cache
contents may
be lost in a system crash or power failure.  In order to provide
reasonable assurances of file integrity, writes to disk probably cannot
be delayed more than a few seconds or minutes.  Thus the write traffic
will limit the overall performance improvements from caching to about
a factor of three or four.

Using the estimates of the previous section, caching should
reduce disk requirements to 1-5 per user, assuming
100-MIP workstations.  The low end of this range might be tolerable,
but the high end would still result in the disk drives being the
greatest part of a workstation's cost.

Although file caching appears unlikely to solve all the I/O
problems by itself, it changes the nature of disk I/O in two
ways.
First, it shifts the balance of disk I/O from mostly reads to mostly
writes.  Second, caches provide a buffer for bursts
of I/O (and I/O is notoriously bursty).  Bursts of reads will mostly be
satisfied in the cache;  bursts of writes can be buffered in the cache,
with the actual disk I/O performed asynchronously without requiring the
writing process to wait.

At this point, two interesting issues emerge:
\begin{enumerate}
\item Disk traffic in the future will become more and more dominated
by writes, and most of the data written will never be read back (it
will live in the file cache until deleted or overwritten).  The main
reason for performing disk I/O is as a safety precaution in case
the cache contents are lost.
\item Seeks limit disk performance.  If files are only 3-4 kbytes long
and two transfers are required per file, then only 3\% of the
raw disk bandwidth can actually be utilized.
If files continue to be small on average, then
the only hope for improving disk bandwidth is to reduce the
number of seeks to much less than one per file.
\end{enumerate}

Further improvements in file system performance may be obtained by addressing
either of these issues.  One general approach is to improve the
reliability of the cache so that its contents can survive crashes
and power failures.  This would make it unnecessary to write
data to disk until it is replaced in the cache.    The next section suggests possible ways to
improve cache reliability.  The second general approach is to
change the disk organization so that many file accesses may be handled
with a single seek.  Section~\ref{logfs} introduces log-structured file systems
as an approach along this line.

\section{Reliable Caches}
\hskip\parindent
\label{nonvol-cache}

One way to
make file caches more reliable is to store them in
memory with battery backup, or on detachable ``silicon disks''.  
This would allow the contents of the
caches to survive power outages.
Another way to improve the reliability of file caches is to use
one or more disks as a ``cache log'', which would hold a backup copy
of cached blocks.  As blocks in 
the cache are modified, their new contents could be written out
to the backup disk in a sequential stream.  Either of these two
approaches could be used to reduce the rate of nonsequential disk
I/O substantially.

If file caches
become so reliable that information need not 
be written through to disk, then the primary role of the file system's
disks would be as an 
archival storage 
area for infrequently-accessed data.  In such an environment, we
wonder whether current disk organizations are still appropriate.
Perhaps it would make more sense to organize the disk structures around
their archival function; i.e., with support for mass storage and
version retention, and with enough
redundancy to eliminate the tape backups required in today's systems.


\section{A Log-Structured File System}
\hskip\parindent
\label{logfs}
One attractive possibility is to merge the cache log with
the main file disks.  We call this a {\em log-structured file
system\/}, 
because the file system's representation on disk would
consist of nothing but a log.  As files are modified, both file
data and header information are appended to the log in a sequential
stream, without seeks.  As with the cache-logging approach described
above, this allows the file system to utilize the full bandwidth of
a disk array, even if individual files are small (they can be
collected into large blocks before being written to the log).
Unlike the cache-logging approach, though, it uses only a single
representation of information on disk.

In addition to their potential for improved disk bandwidth utilization,
log-structured file systems have other attractive
properties:
\begin{itemize}
\item {\bf Fast recovery}. Log-structured file systems offer the possibility
of much faster crash recovery than traditional random-access file systems.
File systems that perform update-in-place, such as UNIX
file systems, can leave the on-disk structures inconsistent if a power-fail
or system crash occurs during a complex update.  As part of
rebooting the operating system must scan all of the disk maps and free
lists in order to detect and repair these inconsistencies.
In contrast, all of the recently-changed information in a
log-structured file system is at the head of the log;  this results in
cleaner failure modes.
\item {\bf Versioning}. The append-only nature of the log means that
old versions of files are retained on disk.  With a little extra
bookkeeping effort, it should be possible to make these old versions
available to users.  Old versions of files could potentially be retained
permanently 
in slower mass storage, along with files that have been unreferenced
for an extended period.
\end{itemize}

A number of difficult issues must be resolved to make
log-structured file systems practical; unfortunately, space permits
discussion  of only the most important of these issues: random
retrieval from the log, and log wrap-around.  Section~\ref{retrieval}
describes 
an approach that allows files to be retrieved from the log
as quickly as today's UNIX systems, and  Section~\ref{wrap} discusses
log wrap-around. 

\subsection{Random Retrieval}
\hskip\parindent
\label{retrieval}
Although we expect caches to reduce disk reads drastically,
a log-structured file system must still provide a mechanism for
retrieving information from the log.  Most systems using typical
database logging techniques would
require the log to be scanned sequentially to recover information from
it.  This would result in read performance thousands of times worse than
today's file systems;  even a very small number of reads could ruin the
overall performance of such a system.
Thus a random-access retrieval mechanism seems essential if a
log-structured file system is to provide improved overall performance.

In a traditional file system such as 4.3 BSD UNIX~\cite{mckusick:unix42}
there are two steps in locating a file.  First, its textual
name must be translated into an internal identifier for the file.
Second, given the internal identifier, the blocks of the file
must be located.  Name-to-identifier translation is accomplished
by reading directory files (starting from a root or
working directory whose identifier is known), so the whole
problem reduces to locating the blocks of a file, given its identifier.
Most systems accomplish this by using a map for each file, which describes
where the file's blocks are located.
For example, UNIX file systems place map information in a few
fixed locations on disk.  The internal identifier used to identify a
file indicates where its map is located.

Unfortunately, the log approach doesn't permit maps in
fixed locations, since this would require seeks to modify the
maps whenever files are created, modified, or deleted.  Instead,
all new or modified information, whether data
or map, must be written at the head of the log.
Thus the key to providing random retrieval is to design a floating
map structure that is integrated with the rest of the log.  The
paragraphs below describe one solution to this problem.
It is not the only possible solution (see~\cite{gait:ofc,garfinkel:cdfs,finlayson:log-files} for other examples),
but it illustrates the feasibility of
random retrieval in a log-structured file system.

\begin{figure*}[t]
 \begin{center}
\fbox{ \vbox{
  \pspicture{/sprite2/users/douglis/sprite/logfs/f1.ps}{4in}}}
\caption{The derivation of a log-structured file system from
a traditional one.  Each figure shows the addition of a block to an
existing file (``before'' is above and ``after'' is below).
(a) shows a traditional file system with separate
map and data areas;  a new data block is allocated and
the map is updated in place.  In (b) the data area has been made into
a log:  each new data block gets added at the end of the log, but map
entries are still updated in place.  In (c) the whole disk is a log.
The map array is treated
like a file whose location is determined by a super-map;  when a file
is modified, a new copy of its map is appended to the head of the log
and the super-map is modified to show the location of that map.
In (d) the super-map
is also written to disk to permit fast recovery after crashes.}
\label{derivation}
\end{center}
\normalsize
\end{figure*}

Figure~\ref{derivation} illustrates in three steps how a traditional
file system 
with a single fixed-location map can be turned into a log-structured
file system with floating maps.  Figure~\ref{derivation}(a) shows a traditional
disk, with a map array in one portion of the disk and file data in
another.  Figure~\ref{derivation}(b) shows the first step towards a log-structured file
system.  New data blocks are now allocated sequentially from the 
head of the log;  old data blocks are not re-used after deletion
(but Section~\ref{wrap} below discusses how they might be re-used when the log
wraps around).  In Figure~\ref{derivation}(b), however, the map array is still in a
fixed location and must be updated in place whenever the file
structure changes.

Figure~\ref{derivation}(c) shows the second step towards a log-structured
file system.  The map array is now treated as a special file:  the
data blocks of this special file contain the maps.  By treating
the map array as a file, its blocks can float just like any other
file:  when a map entry is modified, its block of the
map file is rewritten at the head
of the log.  An additional ``super-map'' gives the location
of each block in the map file.  Each reference to a map entry must
consult the super-map to locate the entry on disk, but the super-map
can be cached in memory just like all other file information,
so no extra disk accesses are required.

The final step consists of occasionally writing the super-map to the
head of the log.  This
is illustrated in Figure~\ref{derivation}(d).  The super-map should contain information
that identifies it unambiguously as such.  After a system crash, all
that is needed to recover the file system is to search back through
the log to the most recent copy of the super-map.  The super-map should
be written often enough that this is a short search.  Then the log
entries after the super-map can be reprocessed to regenerate the
system state at the time of the crash.  Writing the super-map to disk
is something like an atomic file system commit:  it encapsulates
the state of the system at a particular point in time.

The culmination of all these steps is a file system
with read performance nearly identical to current UNIX file
systems.  At the same
time, it makes possible the performance advantages that come from
using a log approach to write data.

\subsection{Log Wrap-Around}
\hskip\parindent
\label{wrap}

The second major issue in building a log-structured file system
is how to handle wrap-around.  No matter how large the log is, it
will eventually fill up.  By this time most of the information in
the log will no longer be ``alive'':  most of the files will have
been deleted or overwritten.  The issue,
then, is how to recover the 
space that is no longer used while continuing to access the disk
as a log.

The issues in dealing with log wrap-around are similar to the issues
in garbage collection.
One possibility is the stop-and-copy approach:  stop the
system and copy all the live
information in the log to one end, leaving the free space all together
at the other end.  Although some write-once file systems use this
approach~\cite{gait:ofc,garfinkel:cdfs}, it seems infeasible to us because
it would result in lengthy down-times for compaction.  Because
logs of practical size might wrap around in less than a day, it is
unreasonable to expect  that
compaction could  be carried out only at night.

% \begin{figure*}[t]
%  \begin{center}
% \fbox{ \vbox{
%   \pspicture{/sprite2/users/douglis/sprite/logfs/f2.ps}{4in}}}
% \caption{Incremental compaction in a log-structured file
% system.  New data is appended to the log from left-to-right.
% In (a) the log has just filled up;  among the oldest
% blocks in the log, only a few are still alive.  In (b) the live
% information is compacted to the head of the log, leaving empty
% space for new log information.
% In (c) new information is appended to the log, regenerating the
% situation in (a).}
% \label{compaction}
% \end{center}
% \normalsize
% \end{figure*}

An alternative is to compact the log incrementally and
continuously, so that the log is continually wrapping on itself.
Before each new portion of the log is written,
the old information occupying the space must be examined.
Dead information may be discarded, but live information must be
saved by copying it, skipping it, or archiving it.

One possible approach is to compact the live information by
copying it to the head of the log.
This approach will result in extra
expense for long-lived files that get recopied every time the log
wraps past them, even though they aren't being used.  A second
approach is to leave the live information in place and add new
information around it.  This approach might result in extra overhead
to skip over the live information, and it would cause
the disk to become fragmented,
so it is probably a bad idea except for very large blocks of live
information.
A third
approach is to move some or all of the live information to another
storage area.  For example, there might be a hierarchy of logs where
information gets archived from log to log as the logs wrap.
This approach produces a result similar to garbage collectors
based on generation scavenging~\cite{ungar:gen-scav}.
It offers the possibility of integrating archival storage (such as
optical disks or digital video-cassettes)
into the system as the most senior level in the log
hierarchy.

% Log wrapping introduces an overhead factor of two in disk bandwidth
% utilization:  each log block must be read (to recover live information)
% before it is written.  A disk seek might also be required to
% reposition the disk heads before writing.  However the distance of
% such seeks will be small, and the log can be written in very large
% blocks, so the seek will be amortized over a large amount of data.
 
% \subsection{Disk Utilization}
% \hskip\parindent
% \label{utilization}
% 
% The third problem in implementing a log-structured file system
% is disk space utilization.
% Log wrap-around introduces a time-space
% tradeoff between the efficiency of disk bandwidth utilization and
% the efficiency of disk space utilization.  Figure~\ref{util-fig}
% illustrates the problem with some hypothetical scenarios.
% In general, higher disk space utilization implies that more
% information will still be alive when the log wraps around it,
% which means that more of the disk's bandwidth will be used
% to copy that information and less bandwidth will be available
% to write out new information.
% \begin{figure*}[t]
%  \begin{center}
% \fbox{ \vbox{
%   \pspicture{/sprite2/users/douglis/sprite/logfs/f3.ps}{4in}}}
% \caption{Some hypothetical scenarios for disk space utilization
% in a log-structured file system.
% In each figure, the x-axis corresponds to location in the log:  \bf 0\/\rm  corresponds to the information just written, and \bf Wrap\/\rm  corresponds to the oldest information in the log,
% which is about to be overwritten.
% The y-axis indicates the fraction of the information at that position
% in the log that is still in use:  older information is more likely to
% have been deleted.  The total disk space utilization is the ratio of the
% area under the curve to the area of the dotted rectangle.  The work
% required during wrapping is closely related to the height of the
% curve at the \bf Wrap\/\rm  point.  In (a) most information has been deleted
% by the time the log wraps on it;  this results in low overhead during
% log wrapping, but poor disk utilization.  In (b) the disk space is
% almost fully utilized, but most of the disk bandwidth will be used to
% recopy information at the log head.  (c) shows an ideal but
% unrealistic situation
% where files live until just before they are wrapped upon, then die
% at the last second so that they need not be dealt with at wrapping
% time.}
% \label{util-fig}
% \end{center}
% \normalsize
% \end{figure*}
% 
% Two factors determine the precise nature of the tradeoff.  The
% first is the distribution of file lifetimes, which is more like
% Figure~\ref{util-fig}(a) than Figure~\ref{util-fig}(c).  This factor
% is outside the control 
% of the file system.  The second factor is overall disk space
% utilization, which is under the file system's control.
% The file system can control space utilization by setting a
% threshold and refusing to allocate new files whenever
% the space utilization exceeds the threshold (for example,
% in 4.3 BSD UNIX, the threshold is 90\%).  This makes
% it possible to select any of a range of scenarios between the
% curves in Figure~\ref{util-fig}(a) and Figure~\ref{util-fig}(b).  Table I shows the
% range of tradeoffs that will apply if file lifetimes are
% exponentially-distributed.  For example, the second line in
% the table indicates that if users are willing to pay twice as much
% for disk storage (i.e., they only use half the available space),
% then they should be able to improve performance by a
% factor of 10 (40\% bandwidth utilization instead of 3\% in today's
% non-log-structured file systems).
% 
% \begin{table}
%  \begin{center}
%   \begin{tabular}{|l|l|l|}
% \hline
% \multicolumn{1}{|c|}{Live}&\multicolumn{1}{c|}{Bandwidth}&\multicolumn{1}{c|}{Space} \\
% \multicolumn{1}{|c|}{Fraction}&\multicolumn{1}{c|}{Utilization}&\multicolumn{1}{c|}{Utilization}\\
% \hline
% .1&.45&.39\\
% .2&.40&.50\\
% .3&.35&.58\\
% .4&.30&.65\\
% .5&.25&.72\\
% .6&.20&.78\\
% .7&.15&.84\\
% .8&.10&.90\\
% .9&.05&.95\\
% 	 \hline
% \end{tabular}
% \caption{The tradeoff between disk space utilization and
% disk bandwidth utilization.  The first column lists the
% fraction of information still alive when the log wraps on it.
% The second column lists the fraction of raw disk bandwidth that
% can be utilized to write new information
% (this assumes that half the bandwidth is used to read back the old
% contents of the log;  of the remaining half, some is used to rewrite
% live information and the rest is used to write new information).
% The third column contains disk space utilization, computed under the
% assumption of exponentially-distributed file lifetimes.
% }
% \label{util-tradeoffs}
%  \end{center}
% 
% \end{table}

\section{Related Work}
\hskip\parindent
\label{related}
The database community has used logging techniques for many years,
both to reduce disk I/O during transaction processing and as a reliability
and crash-recovery tool.  However, in databases the log is
a supplement to the random-access representation of the database,
not a replacement for it as in a log-structured file system.

Logging approaches have begun to see some use in general-purpose
file systems, but the motivation has usually been reliability or
the limitations of write-once media.  In contrast, our motivation
for logging is to achieve high performance with read-write media.
At least three recent projects have used logging as part of
a file system for write-once media:
Swallow~\cite{svobodova:sosp-swallow}, CDFS~\cite{garfinkel:cdfs}, and 
the Optical File Cabinet~\cite{gait:ofc}.  The Swallow system also included
a generational approach to handle media with different characteristics,
and the CDFS and the Optical File Cabinet papers describe mechanisms for
random-access retrieval from their logs.  These systems assumed
infinite storage capacity or write-once media, so they did not address
wrap-around issues.

Finlayson and Cheriton have recently implemented a system providing
append-only log files and propose them as a general-purpose construct~\cite{finlayson:log-files}.
Finlayson and Cheriton's system allows retrieval from the log with
cost proportional to the logarithm of the log size (in comparison to
the constant cost of the mechanism in Section~\ref{retrieval}).  However,
Finlayson and Cheriton required all files to be append-only,
rather than the more general UNIX-like model we have assumed, and
they also assumed infinite storage capacity
and did not have to address log wrap-around issues.

One project where logging has been used to improve performance is
Hagmann's work on the Cedar File
System~\cite{hagmann:cedar-file-system}.  He used logs for file 
maps in order to provide high reliability of map information without
performance degradation.  However, Hagmann used logs only for the
map information, so his mechanisms did not reduce seeks for file data.
Furthermore, in Hagmann's work the log was a supplement to the
normal on-disk structures, not a replacement for it.

\section{Summary and Conclusions}
\hskip\parindent
\label{concl}

If CPU speeds increase by a factor of 100-1000 between 1985 and
1995, then corresponding increases in I/O speeds must be found so
that applications can scale smoothly in performance.
We have described several possible solutions to this problem
and think that log-structured file systems offer a particularly
intriguing alternative for engineering and office applications.
Log-structured file systems can make use of disk arrays
to provide efficient writing, even for small files, and work well
with main-memory file caches, which provide efficient reading.
As a result, we think that overall I/O performance improvements of a factor of
1000 are feasible.  Most of the improvement will come from having a
hundred or so disks in an array;  this provides the potential for a
hundred-fold improvement in bandwidth or operations per second, if the
operating system can keep all of the disks busy.  Log-structured file
systems will
allow all of the disks to be utilized, and also provide about
ten-times better bandwidth utilization of each disk than today's file
systems.




% Even if one of the other approaches, such as file caches with battery
% backup, proves better than log-structured file systems, we think that
% the nature of I/O to disk is changing enough to justify new disk
% organizations.  Logging approaches offer easier recovery and
% versioning and better locality than most of today's file systems,
% so some of the logging techniques may be applicable as a supplement
% to other approaches.
% 
\section{Acknowledgments}
\hskip\parindent

Garth Gibson, Doug Johnson, Randy Katz, Mike Nelson,
Mike Stonebraker, and Herv\'e Touati provided comments on
early drafts of this paper, which substantially improved the
presentation.

%The work described here was supported in part by the
%National Science Foundation under Presidential Young
%Investigator Award No. ECS-8351961 and Grant No. MIP-8715235.
%
\small
\bibliography{os,sosp11}
\bibliographystyle{ieeetr}
\end{document}

